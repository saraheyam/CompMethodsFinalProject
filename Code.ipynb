{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X-Ray: Anomaly-Detection and Classification\n",
    "\n",
    "### Computational Methods II Final Project\n",
    "\n",
    "#### Sarah Yam and Joseph Hostyk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import inspect\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean our data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels:\n",
    "def getPatientInfo(patientDataFile):\n",
    "    \n",
    "    patientDataDict = defaultdict(dict)\n",
    "    classificationsToPatients = defaultdict(set)\n",
    "    \n",
    "    with open(patientDataFile, \"r\") as patients:\n",
    "        header = patients.readline().strip().split(\",\")\n",
    "        for patient in patients:\n",
    "            \n",
    "            # We're only keeping frontal (78k), leaving out lateral (10k)\n",
    "            \n",
    "            if \"lateral\" not in patient:\n",
    "            \n",
    "                patient = dict(zip(header, patient.strip().split(\",\")))\n",
    "                ID = patient[\"Patient ID\"]\n",
    "                path = patient[\"Path\"]\n",
    "                \n",
    "                classificationRow = [column for column in patient if patient[column] == \"1\" and column != \"Sum per Row\"]\n",
    "\n",
    "                # Might be 'No Finding' and 'Support Devices'. In which case, we want the last option.\n",
    "                # Otherwise, there should just be one label. In which case we want the last option.\n",
    "                try:\n",
    "                    classification = classificationRow[-1]\n",
    "                    if len(classificationRow) > 1 and classificationRow != ['No Finding', 'Support Devices']:\n",
    "                        print(classificationRow)\n",
    "                    patientDataDict[ID][\"Path\"] = path\n",
    "                    patientDataDict[ID][\"Classification\"] = classification\n",
    "                    \n",
    "                    classificationsToPatients[classification].add(ID)\n",
    "\n",
    "                # The final row of the file is just a tally of the columns.\n",
    "                except IndexError as e:\n",
    "                    continue\n",
    "                    \n",
    "    return patientDataDict, classificationsToPatients\n",
    "\n",
    "###### will run tests using my set 0 partition (I made 10 partitions, so each set has about ~8k images)\n",
    "###### partitions were done based on the last digit in the patient ID [0 - 9], so it's mildly randomized\n",
    "\n",
    "###### current file path to set0 objects: ./set0/Subsetted/train/patients[0 - 9]\n",
    "###### file path to set0 patient data: ./Subset0.csv \n",
    "\n",
    "###### file path to my actual, full subset: ./SubPatients/train/patients[0 - 9]\n",
    "###### name of datafile in my directory (I'll push it later today): SubsetPatients.csv\n",
    "\n",
    "patientDataFile = \"Subset0.csv\"\n",
    "#patientDataFile = \"SubsetPatients.csv\"\n",
    "allPatientsDataDict, classificationsToPatients = getPatientInfo(patientDataFile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get labels:\n",
    "def getPatientInfo(patientDataFile):\n",
    "    \n",
    "    i = 0\n",
    "    classToIndex = defaultdict(set)\n",
    "    patientDataDict = {}\n",
    "    \n",
    "    with open(patientDataFile, \"r\") as patients:\n",
    "        header = patients.readline().strip().split(\",\")\n",
    "        \n",
    "        \n",
    "        for patient in patients:\n",
    "\n",
    "        # We're only keeping frontal (78k), leaving out lateral (10k)\n",
    "        # Want {idx: ID, Path, Classification} per patient\n",
    "\n",
    "            if \"Lateral\" not in patient:\n",
    "\n",
    "                patient = dict(zip(header, patient.strip().split(\",\")))\n",
    "                \n",
    "                idx = i\n",
    "                ID = patient[\"Patient ID\"]\n",
    "                path = patient[\"Path\"]\n",
    "                classificationRow = [column for column in patient if patient[column] == \"1\" and column != \"Sum per Row\"]\n",
    "                \n",
    "                # Might be 'No Finding' and 'Support Devices'. In which case, we want the last option.\n",
    "                # Otherwise, there should just be one label. In which case we want the last option.\n",
    "                try:\n",
    "                    classification = classificationRow[-1]\n",
    "                    if len(classificationRow) > 1 and classificationRow != ['No Finding', 'Support Devices']:\n",
    "                        print(classificationRow)\n",
    "                    \n",
    "                    # threw in the index because it's the only other unique identifier aside from Path\n",
    "                    # using idx instead of ID for class to Patients\n",
    "                    \n",
    "                    patientDataDict[i] = ID, path, classification\n",
    "                    classToIndex[classification].add(idx)\n",
    "                    \n",
    "                    # increase index for each new entry being used \n",
    "                    i += 1\n",
    "                    \n",
    "                # The final row of the file is just a tally of the columns.    \n",
    "                except IndexError as e:\n",
    "                    continue\n",
    "             \n",
    "             \n",
    "                \n",
    "    return patientDataDict, classToIndex\n",
    "\n",
    "\n",
    "# will run tests using my set 0 partition (I made 10 partitions, so each set has about ~8k images)\n",
    "# partitions were done based on the last digit in the patient ID [0 - 9], so it's mildly randomized\n",
    "\n",
    "# current file path to set0 objects: ./set0/Subsetted/train/patients[0 - 9]\n",
    "# file path to set0 patient data: ./Subset0.csv \n",
    "\n",
    "# file path to my actual, full subset: ./SubPatients/train/patients[0 - 9]\n",
    "# name of datafile in my directory (I'll push it later today): SubsetPatients.csv\n",
    "\n",
    "patientDataFile = \"Subset0.csv\"\n",
    "patientDataDict, classToIndex = getPatientInfo(patientDataFile)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split each disease into 80/20\n",
    "\n",
    "def splitData(classToIndex):\n",
    "    \n",
    "    trainIndices = []\n",
    "    testIndices = []\n",
    "    \n",
    "    trainPercentage = 0.8\n",
    "    \n",
    "    for classification, patientsWithThatClassification in classToIndex.items():\n",
    "        \n",
    "        total = len(patientsWithThatClassification)\n",
    "        trainAmount = int(trainPercentage * total)\n",
    "        \n",
    "        print(\"There are {} in {}.\".format(total, classification))\n",
    "        \n",
    "        hm = list(patientsWithThatClassification)\n",
    "        \n",
    "        random.shuffle(hm)\n",
    "        \n",
    "        trainIndicesForThisClassification = hm[:trainAmount]\n",
    "        testIndicesForThisClassification = hm[trainAmount:]\n",
    "        \n",
    "        \n",
    "        trainIndices += trainIndicesForThisClassification\n",
    "        testIndices += testIndicesForThisClassification\n",
    "        \n",
    "    return trainIndices, testIndices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 308 in Fracture.\n",
      "There are 6112 in Support Devices.\n",
      "There are 810 in Atelectasis.\n",
      "There are 285 in Lung Lesion.\n",
      "There are 192 in Pneumonia.\n",
      "We're left with 6164 train and 1543 test.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This could be an area of confoundment. Since there's patients with more than one image, \n",
    "using patientIDs as the identifier could possibly cause two classifications to map to one patient ID.\n",
    "It's unlikely, but it's definitely something to eventually consider.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "trainIndices, testIndices = splitData(classToIndex)\n",
    "print(\"We're left with {} train and {} test.\".format(len(trainIndices), len(testIndices)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index to patient matching\n",
    "\n",
    "def indexToPatient(indexSet, patientDataDict):\n",
    "\n",
    "    index_patient = [[0,0]] * len(indexSet)\n",
    "\n",
    "    for i in range(len(indexSet)):\n",
    "\n",
    "        patientIndex = indexSet[i]\n",
    "        matchedID = patientDataDict[patientIndex][0]\n",
    "        \n",
    "        index_patient[i] = (patientIndex, matchedID)\n",
    "        \n",
    "    return index_patient\n",
    "\n",
    "trainIndexToPatient = indexToPatient(trainIndices, patientDataDict)\n",
    "testIndexToPatient = indexToPatient(testIndices, patientDataDict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write them out:\n",
    "import csv \n",
    "\n",
    "with open(\"trainPatients.txt\", \"w\") as out:\n",
    "    rowbyrow = csv.writer(out)\n",
    "    rowbyrow.writerows(trainIndexToPatient)\n",
    "\n",
    "with open(\"testPatients.txt\", \"w\") as out:\n",
    "    rowbyrow = csv.writer(out)\n",
    "    rowbyrow.writerows(testIndexToPatient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attempt 2 with a pandas dataframe instead\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    '''Image Label Data as Dataset Object'''\n",
    "    \n",
    "    def __init__(self, indices, patientDataFrame):\n",
    "        '''\n",
    "        Args:\n",
    "            patientID (string): ID's for patients within data subset\n",
    "            patientDataFrame (dataframe): ID, Path, Classification\n",
    "        \n",
    "        '''\n",
    "        self.indices = indices\n",
    "        self.dataframe = patientDataFrame\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        # add 1 for non-length measurement\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # individualName = self.dataframe.iloc[index,0]?\n",
    "        \n",
    "        # get ID, Classification, and Path per entry\n",
    "        # use path to open corresponding image to entry (NOT corresponding to ID)\n",
    "        individualName = self.indices[index]\n",
    "        individualClass = self.dataframe.iloc[individualName][\"Classification\"]\n",
    "        individualPath = self.dataframe.iloc[individualName][\"Path\"]\n",
    "        \n",
    "        image = Image.open(individualPath)\n",
    "        pixelArray = np.array(image)\n",
    "        \n",
    "        patientInfo = {\"classification\": individualClass, \"image\": pixelArray}\n",
    "        \n",
    "        return patientInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to make referencing easy. Continue utilizing idx as unique indexer\n",
    "\n",
    "patientDataFrame = pd.DataFrame.from_dict(patientDataDict, orient = 'index')\n",
    "patientDataFrame.columns = ('ID', 'Path', 'Classification')\n",
    "\n",
    "train_dataset = ImageDataset(trainIndices, patientDataFrame)\n",
    "test_dataset = ImageDataset(testIndices, patientDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So it's really not having it with the dicts apparently. The only attributes that I could determine it had were .names and .patientInfo, which actually only just returned the original dict itself, and isn't easily callable. I'm not entirely sure how the images are getting stored or under what attribut name if any. I'm going to try doing this by creating a pandas from the dict.\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\" dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, names, allPatientsDataDict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            names (string): names\n",
    "        \"\"\"\n",
    "        self.names = names\n",
    "        self.patientInfo = allPatientsDataDict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        individualName = self.names[index]\n",
    "        \n",
    "        # going to try linking via index\n",
    "        individualDisease = self.patientInfo[individualName][\"Classification\"]\n",
    "        individualPath = self.patientInfo[individualName][\"Path\"]\n",
    "        \n",
    "        image = Image.open(individualPath)\n",
    "        pixelArray = np.array(image)\n",
    "\n",
    "        patientInfo = {\"classification\": individualDisease, \"image\": pixelArray}\n",
    "\n",
    "        return patientInfo\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SO FUN FACT:\n",
    "#### All of our images are slightly different sizes and the neural network we built isn't flexible enough to handle that, at least not that I can think of right off the top of my head. We'll either have to resize them via PIL as a \"transform\" or find a way to dynamically take in images of different sizes.\n",
    "\n",
    "##### Among other options beyond PIL was a sort of informative stackoverflow link I found: https://stackoverflow.com/questions/41907598/how-to-train-images-when-they-have-different-size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 6164\n",
      "Test data size: 1543\n",
      "Train: Fracture\n",
      "\t Image shape: (320, 389)\n",
      "\t Image size: 124480\n",
      "Test: Fracture\n",
      "\t Image shape: (320, 320)\n",
      "\t Image size: 102400\n",
      "Train: Fracture\n",
      "\t Image shape: (320, 368)\n",
      "\t Image size: 117760\n",
      "Test: Fracture\n",
      "\t Image shape: (320, 355)\n",
      "\t Image size: 113600\n",
      "Train: Fracture\n",
      "\t Image shape: (320, 390)\n",
      "\t Image size: 124800\n",
      "Test: Support Devices\n",
      "\t Image shape: (320, 390)\n",
      "\t Image size: 124800\n",
      "Train: Support Devices\n",
      "\t Image shape: (320, 390)\n",
      "\t Image size: 124800\n",
      "Test: Support Devices\n",
      "\t Image shape: (320, 389)\n",
      "\t Image size: 124480\n",
      "Train: Support Devices\n",
      "\t Image shape: (320, 390)\n",
      "\t Image size: 124800\n",
      "Test: Support Devices\n",
      "\t Image shape: (320, 389)\n",
      "\t Image size: 124480\n",
      "Train: Support Devices\n",
      "\t Image shape: (320, 390)\n",
      "\t Image size: 124800\n",
      "Test: Support Devices\n",
      "\t Image shape: (320, 390)\n",
      "\t Image size: 124800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNo idea if the difference in sizes for the different classifications of the images is going to be a good thing or not.\\nIt might make is easier for the learning, but it alsoc ould be more confounding. Who knows.\\n'"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the data\n",
    "'''\n",
    "MNIST dataset stucture was a priori split into training and testing. Because we don't have data that's pre-defined like \n",
    "that, or anywhere near that structure, we have to call it in a different manner. MNIST already has its labels connected to its images for both training and \n",
    "testing, so they already have pre-defined sizes/attributes like train_data.size or train_label.size.  Apparently, the \n",
    "only way to do this is by checking the sizes of individual samples. So I'll run through a few here and make sure they're\n",
    "for the most part consistent (hopefully).\n",
    "'''\n",
    "\n",
    "train_length = len(train_dataset)\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "print(\"Train data size:\", train_length)\n",
    "print(\"Test data size:\", test_length)\n",
    "\n",
    "for i in [4, 13, 69, 420, 666, 888]:\n",
    "    train_entry = train_dataset[i]\n",
    "    test_entry = test_dataset[i]\n",
    "    \n",
    "    print(\"Train:\", train_entry['classification'])\n",
    "    print(\"\\t Image shape:\", train_entry['image'].shape)\n",
    "    print(\"\\t Image size:\", train_entry['image'].size)\n",
    "    \n",
    "    print(\"Test:\", test_entry['classification'])\n",
    "    print(\"\\t Image shape:\", test_entry['image'].shape)\n",
    "    print(\"\\t Image size:\", test_entry['image'].size)\n",
    "    \n",
    "'''\n",
    "No idea if the difference in sizes for the different classifications of the images is going to be a good thing or not.\n",
    "It might make is easier for the learning, but it alsoc ould be more confounding. Who knows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average d1 size:  320.1291369240753\n",
      "min: 320\n",
      "max: 390\n",
      "average d2 size:  383.8392277741726\n",
      "min: 320\n",
      "max: 486\n",
      "average size:  122869.87670343932\n",
      "min: 102400\n",
      "max: 155520\n"
     ]
    }
   ],
   "source": [
    "d1 = []\n",
    "d2 = []\n",
    "sizes = []\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    train_entry = train_dataset[i]\n",
    "    \n",
    "    d1.append(train_entry['image'].shape[0])\n",
    "    d2.append(train_entry['image'].shape[1])\n",
    "    \n",
    "    sizes.append(train_entry['image'].size)\n",
    "\n",
    "print(\"average d1 size: \", np.average(d1))\n",
    "print(\"min:\", np.min(d1))\n",
    "print(\"max:\", np.max(d1))\n",
    "print(\"average d2 size: \", np.average(d2))\n",
    "print(\"min:\", np.min(d2))\n",
    "print(\"max:\", np.max(d2))\n",
    "print(\"average size: \", np.average(sizes))\n",
    "print(\"min:\", np.min(sizes))\n",
    "print(\"max:\", np.max(sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 1876\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model.\n",
    "#### SOMETHING SOMETHING DENSE NETWORKS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the code from the Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 1876\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Dynamic CNN\n",
    "The above code is great for getting an idea of how to build a model. But in production we would want to be able to iterate through multiple different architectural choices like number of layers, filter sizes, pooling kernels, strides, activation functions, etc. In order to be able to do this we need to build our model using arguments which specify the architecture and loops. Otherwise we'd have to explicitly type out every layer of our model, for every change we wanted to implement.\n",
    "\n",
    "Now just in case any of your are getting flashbacks to lab 4, not to worry. I've implemented most of the function for you. You're encouraged to walk through it to understand what's happening. **All you have to do is implement the method `calculateFinalOutputSize` that calculates the size of the final output, which is needed to calculate the size of the final FCNN layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModelDynamic(nn.Module):\n",
    "    def __init__(self, input_shape, n_classes,\n",
    "                 in_channels_list, out_channels_list,\n",
    "                 kernel_size_list, stride_list,\n",
    "                 padding_list, pool_kernel_list,\n",
    "                 pool_stride_list,\n",
    "                 pooling_list, activations_list):\n",
    "        super(CNNModelDynamic, self).__init__()\n",
    "        localArgs = locals().items()\n",
    "        argLens = set()\n",
    "        ignoredArgs = ['self', \"__class__\", \"input_shape\", \"n_classes\"]\n",
    "        for argName, arg in localArgs:\n",
    "            if argName not in ignoredArgs:\n",
    "                argLens.add(len(arg))\n",
    "        assert len(argLens) == 1, (\"mismatch in lengths of arguments.\"\n",
    "                                   \"All params for each layer must be specified\")\n",
    "        finalOutputSize = self.calculateFinalOutputSize(input_shape,kernel_size_list, stride_list,\n",
    "                                         padding_list, pool_kernel_list, pool_stride_list)\n",
    "        modules = list()\n",
    "        for layerIdx in range(0, argLens.pop()):\n",
    "            modules.append(nn.Conv2d(in_channels = in_channels_list[layerIdx],\n",
    "                                 out_channels = out_channels_list[layerIdx],\n",
    "                                 kernel_size = kernel_size_list[layerIdx],\n",
    "                                 stride = stride_list[layerIdx],\n",
    "                                 padding = padding_list[layerIdx]))\n",
    "            modules.append(activations_list[layerIdx])\n",
    "            modules.append(pooling_list[layerIdx](kernel_size = pool_kernel_list[layerIdx],\n",
    "                                                  stride = pool_stride_list[layerIdx]))\n",
    "        self.convolutions = nn.Sequential(*modules)\n",
    "        self.finalLayer = nn.Linear(finalOutputSize**2*out_channels_list[-1], n_classes)\n",
    "        \n",
    "    def outputFromConvLayer(self, w, k, p, s):\n",
    "        return (w-k+2*p)/float(s) + 1\n",
    "    \n",
    "    def outputFromPoolLayer(self, w, k, s):\n",
    "        return (w-k)/float(s) + 1\n",
    "    \n",
    "    def calculateFinalOutputSize(self, input_shape, kernel_size_list, stride_list,\n",
    "                                 padding_list, pool_kernel_list, pool_stride_list):\n",
    "        \"\"\"\n",
    "        Calculates the shape of the final output assuming that every conv layer is followed\n",
    "        by a pooling layer.\n",
    "        \"\"\"\n",
    "         #### your code here ###\n",
    "        currentInput = input_shape\n",
    "        for i in range(len(kernel_size_list)):\n",
    "            currentInput = self.outputFromConvLayer(currentInput, kernel_size_list[i], padding_list[i], stride_list[i])\n",
    "            currentInput = self.outputFromPoolLayer(currentInput, pool_kernel_list[i], pool_stride_list[i])\n",
    "        finalOutputShape = currentInput\n",
    "        \n",
    "#         print(\"Final shape is\", int(finalOutputShape))\n",
    "        \n",
    "         #### end code here ###\n",
    "        return(int(finalOutputShape))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.convolutions(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.finalLayer(out)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Model Summary\n",
    "Using the summary function from torchsummary print out the layers, shapes, and number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 24, 24]             416\n",
      "              ReLU-2           [-1, 16, 24, 24]               0\n",
      "         MaxPool2d-3           [-1, 16, 12, 12]               0\n",
      "            Conv2d-4             [-1, 32, 8, 8]          12,832\n",
      "              ReLU-5             [-1, 32, 8, 8]               0\n",
      "         MaxPool2d-6             [-1, 32, 4, 4]               0\n",
      "            Linear-7                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 18,378\n",
      "Trainable params: 18,378\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.19\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 0.27\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#### your code here ###\n",
    "cnnDynamicOG = CNNModelDynamic(input_shape = 28, n_classes = 10,\n",
    "                 in_channels_list = [1, 16], out_channels_list = [16, 32],\n",
    "                 kernel_size_list = [5, 5], stride_list = [1, 1],\n",
    "                 padding_list = [0,0], pool_kernel_list = [2, 2],\n",
    "                 pool_stride_list = [2, 2],\n",
    "                 pooling_list = [nn.MaxPool2d, nn.MaxPool2d], activations_list = [nn.ReLU(), nn.ReLU()])\n",
    "summary(cnnDynamicOG, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Dynamic CNN\n",
    "Now we can train our model. First train the original model implemented above. It shoud train just fine and achieve decent accuracy. \n",
    "\n",
    "1. Your first task is to implement early stopping by keeping track of your best accuracy and stopping training if accuracy doesn't improve after 3 checks. I suggest that you check your evaluation accuracy every 50 iterations unless you have a GPU then by all means check whenever you want. Hell, check twice an iteration. Ain't nothing stopping you with a GPU. Once your best model is found, quit out and stop running. This is where you'd usually save a model, but don't worry about doing that.\n",
    "\n",
    "2. Your second task is to experiment with different configurations. Try at least two architectural changes (depth, convolution layers, channels, strides, **optimization**, average pooling, etc) and record some observations about model performance for your two configurations. I would suggest using Adam as an optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doTheThing(model, optimizer):\n",
    "    numIterations = 0\n",
    "    bestAccuracy = 0\n",
    "    patience = 2 # how many times should we be ok with our accuracy not increasing?\n",
    "    checksWithoutIncrease = 0\n",
    "    num_epochs = 5 # Change later\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Load images\n",
    "            images = images.requires_grad_()\n",
    "\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass to get output/logits\n",
    "            outputs = model.forward(images)\n",
    "\n",
    "            # Calculate Loss: softmax --> cross entropy loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Getting gradients w.r.t. parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            numIterations += 1\n",
    "            if numIterations % 50 == 0:\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                # Iterate through test dataset\n",
    "                for images, labels in test_loader:\n",
    "                    # Load images\n",
    "                    images = images.requires_grad_()\n",
    "\n",
    "                    # Forward pass only to get logits/output\n",
    "                    outputs = model.forward(images)\n",
    "\n",
    "                    # Get predictions from the maximum value\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                    # Total number of labels\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                    # Total correct predictions\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "                accuracy = 100 * correct / total\n",
    "                # Check if early stopping criteria are met\n",
    "                #### your code here ###\n",
    "\n",
    "                if accuracy > bestAccuracy:\n",
    "                    bestAccuracy = accuracy\n",
    "                    checksWithoutIncrease = 0\n",
    "                else:\n",
    "                    checksWithoutIncrease += 1\n",
    "                    print(\"\\tGone {} rounds without increasing:\".format(checksWithoutIncrease))\n",
    "\n",
    "                #### end code here ###\n",
    "                print('\\tIteration: {}. Loss: {}. Testing Accuracy: {}'.format(numIterations, loss.item(), accuracy))\n",
    "        # remember, early stopping is qutting out of all training.\n",
    "        #### your code here ###\n",
    "                if checksWithoutIncrease > patience:\n",
    "                    print(\"We did not increase accuracy over the past 3 rounds. Quitting!\")\n",
    "                    return bestAccuracy, numIterations * (epoch + 1)\n",
    "        #### end code here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnDynamicOG = CNNModelDynamic(input_shape = 28, n_classes = 10,\n",
    "                 in_channels_list = [1, 16], out_channels_list = [16, 32],\n",
    "                 kernel_size_list = [5, 5], stride_list = [1, 1],\n",
    "                 padding_list = [0,0], pool_kernel_list = [2, 2],\n",
    "                 pool_stride_list = [2, 2],\n",
    "                 pooling_list = [nn.MaxPool2d, nn.MaxPool2d], activations_list = [nn.ReLU(), nn.ReLU()])\n",
    "\n",
    "# we'll tweak this around\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "optimizerSGD = torch.optim.SGD(cnnDynamicOG.parameters(), lr=learning_rate)\n",
    "optimizerAdam = torch.optim.Adam(cnnDynamicOG.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnDynamiclargerKernels = CNNModelDynamic(input_shape = 28, n_classes = 10,\n",
    "                 in_channels_list = [1, 16], out_channels_list = [16, 32],\n",
    "                 kernel_size_list = [8, 8], stride_list = [1, 1],\n",
    "                 padding_list = [0,0], pool_kernel_list = [2, 2],\n",
    "                 pool_stride_list = [2, 2],\n",
    "                 pooling_list = [nn.MaxPool2d, nn.MaxPool2d], activations_list = [nn.ReLU(), nn.ReLU()])\n",
    "optimizerSGDlargerKernels = torch.optim.SGD(cnnDynamiclargerKernels.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 390 and 349 in dimension 2 at /pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1307",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-342-3e1a55c9d78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# bestAccuracyOriginal, totalIterationsOriginal = doTheThing(cnnDynamicOG, optimizerSGD)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# bestAccuracyWithAdam, totalIterationsWithAdam = doTheThing(cnnDynamicOG, optimizerAdam)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbestAccuracyWithLargerKernels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalIterationsWithLargerKernels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoTheThing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnnDynamiclargerKernels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizerSGDlargerKernels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-337-5e6c49bbd0c9>\u001b[0m in \u001b[0;36mdoTheThing\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;31m# Load images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mpy_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 390 and 349 in dimension 2 at /pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1307"
     ]
    }
   ],
   "source": [
    "# bestAccuracyOriginal, totalIterationsOriginal = doTheThing(cnnDynamicOG, optimizerSGD)\n",
    "# bestAccuracyWithAdam, totalIterationsWithAdam = doTheThing(cnnDynamicOG, optimizerAdam)\n",
    "bestAccuracyWithLargerKernels, totalIterationsWithLargerKernels = doTheThing(cnnDynamiclargerKernels, optimizerSGDlargerKernels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With the original settings, the best accuracy achieved was {}, after {} iterations.\".format(bestAccuracyOriginal, totalIterationsOriginal))\n",
    "print(\"Using Adam as the optimizer, the best accuracy achieved was {}, after {} iterations.\".format(bestAccuracyWithAdam, totalIterationsWithAdam))\n",
    "print(\"With a larger kernel size, the best accuracy achieved was {}, after {} iterations.\".format(bestAccuracyWithLargerKernels, totalIterationsWithLargerKernels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
